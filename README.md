# ğŸ§  AI Fabric Hackathon  

## ğŸ¯ Objetivos del Hackathon

Al finalizar este hackathon, los participantes serÃ¡n capaces de:

- Preparar, transformar y enriquecer datos financieros y aseguradores usando **Microsoft Fabric**, aplicando el modelo **medallion** para estructurar capas de valor analÃ­tico.  
- Ingestar datos desde sistemas core, fuentes externas y APIs mediante **pipelines, notebooks y conectores nativos de Fabric**.  
- DiseÃ±ar **modelos semÃ¡nticos** robustos que faciliten el consumo de datos por analistas, auditores y sistemas de inteligencia.  
- Monitorear y optimizar el consumo de capacidad en **Fabric**, aplicando mÃ©tricas clave para gobernanza operativa y eficiencia de recursos.  
- Construir **agentes de inteligencia artificial** con **AI Foundry** para anÃ¡lisis predictivo, detecciÃ³n de fraude y generaciÃ³n de insights financieros.  
- Orquestar flujos multi-agente y procesos de datos con **pipelines y triggers**, habilitando automatizaciÃ³n inteligente en escenarios bancarios y de seguros.  
- Aplicar **controles de seguridad y gobernanza** de datos sensibles, configurando roles, permisos y polÃ­ticas en workspaces de Fabric.  
- Integrar **Microsoft Purview** para trazabilidad, clasificaciÃ³n y cumplimiento normativo, fortaleciendo la gobernanza de datos en entornos regulados.  
- Visualizar **insights estratÃ©gicos** con **Power BI en Microsoft Fabric**, habilitando tableros interactivos para decisiones basadas en datos.  



# Agenda


| DÃ­a  | Actividad                                                                 | Tipo   |
|------|---------------------------------------------------------------------------|--------|
| DÃ­a 1 | PreparaciÃ³n de datos (estructuraciÃ³n, limpieza, perfilado)               | Reto   |
| DÃ­a 1 | Ingesta de datos desde fuentes internas y externas                      | Reto   |
| DÃ­a 1 | TransformaciÃ³n de datos con notebooks y pipelines                        | Reto   |
| DÃ­a 1 | Enriquecimiento de datos y creaciÃ³n de modelo semÃ¡ntico                  | Reto   |
| DÃ­a 1 | Fabric Metrics: monitoreo de capacidad, consumo y rendimiento            | Demo   |
| DÃ­a 1 | Round Table: Q&A con expertos y participantes                            | Reto   |
| DÃ­a 1 | Cierre y resumen del dÃ­a                                                 | Cierre |
| DÃ­a 2 | ConstrucciÃ³n de agente AI Foundry para anÃ¡lisis predictivo               | Reto   |
| DÃ­a 2 | OrquestaciÃ³n multi-agente con pipelines y triggers                       | Reto   |
| DÃ­a 2 | Seguridad en Fabric: roles, objetos, workspaces (opcional)               | Reto   |
| DÃ­a 2 | IntegraciÃ³n con Microsoft Purview: linaje, clasificaciÃ³n, gobernanza     | Demo   |
| DÃ­a 2 | SesiÃ³n de valor: Q&A sobre adopciÃ³n, impacto y prÃ³ximos pasos            | Cierre |
| DÃ­a 2 | Cierre y entrega de reconocimientos                                      | Cierre |


# Arquitectura
![Arquitectura](https://github.com/stmora98/Del_Insight_a_la_Decision/blob/main/Architecture/Architecture.png)


## ğŸ§© 00 - Parte 1: Cargar archivo CSV en Azure Cosmos DB

### ğŸš€ Paso 1: Crear la cuenta de Cosmos DB
1. Ve al **portal de Azure**.  
2. Crea un recurso â†’ **Azure Cosmos DB for NoSQL**.  
3. Asigna nombre, grupo de recursos y regiÃ³n.  
4. Espera a que se aprovisione.  

### âš™ï¸ Paso 2: Crear base de datos y contenedor
1. En tu cuenta de Cosmos DB, ve a **Data Explorer**.  
2. Crea una nueva **base de datos** (ejemplo: `NombreDeLaBase`) y un **contenedor** (ejemplo: `NombreDelContenedor`).  
3. Define una **clave de particiÃ³n** (ejemplo: `/claveParticion`).  
4. Habilita **TTL** si deseas limpieza automÃ¡tica.  

### ğŸ’¾ Paso 3: Insertar JSON en Cosmos DB
1. Ve a **Data Explorer â†’ Contenedor â†’ Items â†’ Upload Item**.  
2. Carga el archivo **JSON** generado.  
3. Verifica que los documentos estÃ©n visibles y bien estructurados.  

---

## â˜ï¸ 01 - Parte 2: Preparar ingesta a Microsoft Fabric con Data Factory  

### GuÃ­a resumida para integrar datos en Microsoft Fabric usando Azure Data Factory  

### ğŸ”§ Paso 1: Crear instancia de Azure Data Factory
- Accede al **portal de Azure** y selecciona **Crear un recurso â†’ Data Factory**.  
- Asigna nombre, grupo de recursos y regiÃ³n.  

### ğŸ”— Paso 2: Crear Linked Services
- **Cosmos DB:** Crea un Linked Service con la clave de acceso.  
- **Fabric Lakehouse:** Configura el conector **OneLake** (token de acceso o conexiÃ³n directa).  

### ğŸ§  Paso 3: Crear pipeline de ingesta
1. Crea un **pipeline** en **Data Factory Studio**.  
2. AÃ±ade actividad **Copy Data**:  
   - **Fuente:** Cosmos DB (selecciona el contenedor).  
   - **Destino:** Fabric Lakehouse (tabla Bronze).  
3. Mapea campos y transforma datos segÃºn sea necesario.  

### âœ… Paso 4: Ejecutar y validar
1. Ejecuta el pipeline.  
2. Verifica que los datos estÃ©n en la tabla **Bronze** de Microsoft Fabric.  
3. Usa **Notebooks Spark** para inspecciÃ³n y transformaciÃ³n adicional.  

---

### âœ… Resultado esperado
- El archivo CSV se carga como documentos **JSON en Cosmos DB**.  
- El pipeline **extrae y carga** datos en Microsoft Fabric.  
- Los datos quedan listos para **transformaciÃ³n en Silver** y **enriquecimiento en Gold**.  

---

## ğŸ”„ 02 â€“ TransformaciÃ³n y unificaciÃ³n  

### ğŸ¯ Objetivo:
Convertir un archivo **JSON** a **CSV** y unificar ambos conjuntos de datos para anÃ¡lisis y procesamiento posterior.  

### ğŸ§­ Paso a paso
1. **Usar notebooks Spark** en Fabric para convertir el archivo JSON a formato CSV.  
   - Abre tu **notebook Spark en Microsoft Fabric**.  
   - Carga el archivo JSON y utiliza funciones de Spark para transformarlo y exportarlo como CSV.  

2. **Realizar una uniÃ³n (join)** de los datasets utilizando el campo **ID de cliente**.  
   - Importa ambos datasets (el original y el convertido) en el notebook.  
   - Utiliza el campo **ID de cliente** como clave para realizar la uniÃ³n y obtener un solo conjunto de datos.  

3. **Guardar el dataset unificado en la capa Silver.**  
   - Una vez completada la unificaciÃ³n, guarda el resultado en la capa Silver de Fabric para futuras transformaciones y enriquecimiento.  

> Al finalizar estos pasos, tendrÃ¡s los datos preparados y listos para procesos avanzados en la capa Gold, asegurando **calidad y consistencia en la informaciÃ³n**.  

---

## ğŸ§¹ 003 - Reto 3: Limpieza y enriquecimiento  

### ğŸ¯ Objetivo:
Limpiar los datos, enriquecer con nuevas columnas y crear un modelo semÃ¡ntico.  

**Pasos:**
- Eliminar duplicados.  
- Normalizar nombres de paÃ­s.  
- Calcular mÃ©tricas:  
  - Total gastado  
  - Frecuencia  
  - PaÃ­s mÃ¡s frecuente  
- Crear **modelo semÃ¡ntico** en la capa Gold utilizando **Power BI o Semantic Model en Fabric**.  

---

## ğŸ¤– 004 - Reto 4: Data Agent en Fabric  

### ğŸ¯ Objetivo:
Crear un agente que responda preguntas sobre los datos.  

**Pasos:**
1. Crear un **Data Agent** en Fabric.  
2. Conectarlo al **modelo semÃ¡ntico**.  
3. Probar el funcionamiento realizando preguntas como:  
   - â€œÂ¿CuÃ¡l es el cliente mÃ¡s frecuente en MÃ©xico?â€  

---

## ğŸ§¬ 005 - Reto 5: Azure AI Foundry + LLMs  

### ğŸ¯ Objetivo:
Aplicar **IA generativa** sobre datos enriquecidos para extraer valor y generar nuevos conocimientos a partir de la informaciÃ³n disponible.  

### ğŸ§© Subretos:

#### ğŸ§¾ Resumen de compras por cliente
**Prompt:**  
> â€œResume el comportamiento de compra del cliente X.â€

#### ğŸ·ï¸ ClasificaciÃ³n de clientes
**Prompt:**  
> â€œClasifica este cliente como alto valor, frecuente u ocasional.â€

#### ğŸ“° Narrativas automÃ¡ticas
**Prompt:**  
> â€œGenera un reporte mensual por paÃ­s con insights narrativos.â€
```markdown
