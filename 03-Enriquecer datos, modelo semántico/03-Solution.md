# üßº **Parte 1: Limpieza de datos**

- **Eliminar duplicados:** Revisa el conjunto de datos y elimina las filas que est√©n repetidas para evitar inconsistencias.  
- **Normalizar nombres de pa√≠s:** Unifica los nombres de los pa√≠ses utilizando un mismo criterio de escritura y formato.  
- **Convertir fechas al formato est√°ndar:** Aseg√∫rate de que todas las fechas sigan el formato **DD/MM/AAAA**.  
- **Eliminar registros nulos o incompletos:** Descarta las filas que contengan datos faltantes o incompletos para mantener la calidad del an√°lisis.

## üß™ **C√≥digo**

```python
# Leer dataset unificado desde Bronze
df = spark.read.csv("Files/dataset_unificado.csv", header=True, inferSchema=True)

# Eliminar duplicados
df_clean = df.dropDuplicates()

# Normalizar nombres de pa√≠s
from pyspark.sql.functions import trim, upper

df_clean = df_clean.withColumn("pais", upper(trim(df_clean["pais"])))

# Convertir fechas al formato est√°ndar
from pyspark.sql.functions import to_date

df_clean = df_clean.withColumn("fecha", to_date(df_clean["fecha"], "yyyy-MM-dd"))

# Eliminar registros con campos nulos cr√≠ticos
df_clean = df_clean.dropna(subset=["customerId", "monto", "fecha"])

# Mostrar resultado limpio
display(df_clean)
```

# üìà **Parte 2: Enriquecimiento de datos**

- **M√©tricas derivadas por cliente**
- **Total gastado:** Suma total de las compras por cliente.
- **Frecuencia de compra:** N√∫mero de compras por cliente en el periodo.
- **Pa√≠s m√°s frecuente:** Pa√≠s desde el que el cliente compra con mayor frecuencia.

## üõ†Ô∏è **C√≥digo**

```Python
from pyspark.sql.functions import sum, count, col, first, desc

# Total gastado y frecuencia
df_metrics = df_clean.groupBy("customerId").agg(
    sum("monto").alias("total_gastado"),
    count("fecha").alias("frecuencia_compra"),
    first("pais").alias("pais_principal")  # simplificado, se puede mejorar con mode()
)

# Unir con datos demogr√°ficos
df_final = df_metrics.join(
    df_clean.select("customerId", "edad", "segmento").dropDuplicates(),
    on="customerId",
    how="left"
)

# Guardar en capa Silver
df_final.write.mode("overwrite").csv("Lakehouse/Silver/dataset_enriquecido.csv", header=True)

# Mostrar resultado enriquecido
display(df_final)
```

# üèÜ **Parte 3: Crear modelo sem√°ntico en capa Gold**

- **Crear tabla Gold desde archivo enriquecido:** Genera una tabla en la capa Gold utilizando el archivo ya enriquecido, asegurando que contiene solo los datos procesados y limpios.  
- **Definir medidas y dimensiones:** Establece claramente las medidas (m√©tricas cuantitativas) y dimensiones (atributos descriptivos) que formar√°n parte del modelo sem√°ntico.  
- **Publicar modelo para Power BI o Data Agent:** Publica el modelo sem√°ntico creado para que est√© disponible y pueda ser consultado desde Power BI o Data Agent seg√∫n las necesidades del negocio.

## üõ†Ô∏è **C√≥digo**
```python
# Leer desde Silver
df_gold = spark.read.csv("Lakehouse/Silver/dataset_enriquecido.csv", header=True, inferSchema=True)

# Guardar como tabla Gold
df_gold.write.mode("overwrite").saveAsTable("Gold.ClienteAnalitico")

# Verificar tabla
spark.sql("SELECT * FROM Gold.ClienteAnalitico LIMIT 10").show()
```

## Resultado esperado
- Datos limpios y enriquecidos listos en la capa Silver.
- Tabla sem√°ntica en la capa Gold con m√©tricas por cliente.
