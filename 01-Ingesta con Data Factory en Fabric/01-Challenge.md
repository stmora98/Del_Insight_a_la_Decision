# ğŸ§© **Reto 1: Ingesta de Datos en Microsoft Fabric con Data Factory**

> **Objetivo:** Preparar e integrar datos desde Azure Cosmos DB hacia Microsoft Fabric utilizando Azure Data Factory.  
> Este proceso establece la base del flujo de datos desde la **capa Bronze** hasta las capas **Silver** y **Gold** para su anÃ¡lisis posterior.

---

## âš™ï¸ **Parte 1: ConfiguraciÃ³n de Azure Data Factory**

### ğŸ”¹ **Paso 1: Crear instancia de Azure Data Factory**
1. Accede al **Portal de Azure**.  
2. Selecciona **Crear un recurso â†’ Data Factory**.  
3. Asigna:
   - Un **nombre** identificativo (ejemplo: `ADF-FabricIngesta`).
   - El **grupo de recursos** correspondiente.
   - La **regiÃ³n** mÃ¡s cercana a tu entorno de Fabric.

ğŸ’¡ *Consejo:* MantÃ©n el nombre coherente con tus recursos de Fabric para facilitar la administraciÃ³n.

---

## ğŸ”— **Parte 2: Configurar Conexiones (Linked Services)**

### ğŸ”¸ **Cosmos DB**
- Crea un **Linked Service** con la clave de acceso proporcionada.  
- Define la conexiÃ³n hacia el **contenedor de datos** donde se almacenan los documentos JSON.

### ğŸ”¸ **Fabric Lakehouse**
- Configura el conector de **OneLake** (requiere token de acceso o conexiÃ³n directa desde Fabric).  
- Verifica la autenticaciÃ³n y permisos para escritura en la tabla destino.

---

## ğŸš€ **Parte 3: Crear el Pipeline de Ingesta**

### ğŸ§± **Pasos detallados**
1. Abre **Azure Data Factory Studio**.  
2. Crea un **nuevo pipeline** y asÃ­gnale un nombre descriptivo (ejemplo: `Pipeline_Ingesta_Cosmos_to_Fabric`).  
3. Agrega una actividad **Copy Data**.  

   **ConfiguraciÃ³n:**
   - **Fuente:** Cosmos DB â†’ selecciona el contenedor correspondiente.  
   - **Destino:** Fabric Lakehouse â†’ elige la tabla en la **capa Bronze**.  
   - **Transformaciones:** mapea los campos y aplica ajustes como:
     - ConversiÃ³n de fechas a formato estÃ¡ndar.
     - NormalizaciÃ³n de campos y tipos de datos.

ğŸ’¡ *Tip:* Puedes agregar un paso de validaciÃ³n para asegurar que el esquema coincida con el esperado en Fabric.

---

## ğŸ§ª **Parte 4: EjecuciÃ³n y ValidaciÃ³n**

1. Ejecuta el pipeline creado desde **Data Factory Studio**.  
2. Verifica en **Microsoft Fabric** que los datos se encuentren disponibles en la tabla **Bronze**.  
3. (Opcional) Usa **Notebooks Spark** dentro de Fabric para:
   - Visualizar los datos.  
   - Validar el formato y consistencia.  
   - Aplicar transformaciones previas al enriquecimiento.

---

## âœ… **Resultado Esperado**

Al finalizar este reto, deberÃ­as obtener:

| Elemento | Resultado |
|-----------|------------|
| **Origen de datos** | Archivo CSV cargado como documentos JSON en **Cosmos DB** |
| **Pipeline** | Configurado y ejecutado correctamente en **Azure Data Factory** |
| **Destino** | Datos disponibles en **Microsoft Fabric (Capa Bronze)** |
| **Siguiente paso** | Los datos listos para transformaciÃ³n en **Capa Silver** y enriquecimiento en **Capa Gold** |

---

ğŸ¯ **ConclusiÃ³n:**  
Con este flujo completado, ya dispones de un **pipeline automatizado** que conecta **Cosmos DB** con **Microsoft Fabric**, sentando las bases para los siguientes retos de transformaciÃ³n, enriquecimiento y anÃ¡lisis de datos.
