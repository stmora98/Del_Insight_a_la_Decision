# üß© **Reto 1: Ingesta de Datos en Microsoft Fabric con Data Factory**

> **Objetivo:** Preparar e integrar datos desde Azure Cosmos DB hacia Microsoft Fabric utilizando Azure Data Factory.  
> Este proceso establece la base del flujo de datos desde la **capa Bronze** hasta las capas **Silver** y **Gold** para su an√°lisis posterior.

---

## ‚öôÔ∏è **Parte 1: Configuraci√≥n des Azure Data Factory**

### üîπ **Paso 1: Crear instancia de Azure Data Factory**
1. Accede al **Portal de Azure**.  
2. Selecciona **Crear un recurso ‚Üí Data Factory**.  
3. Asigna:
   - Un **nombre** identificativo (ejemplo: `ADF-FabricIngesta`).
   - El **grupo de recursos** correspondiente.
   - La **regi√≥n** m√°s cercana a tu entorno de Fabric.

üí° *Consejo:* Mant√©n el nombre coherente con tus recursos de Fabric para facilitar la administraci√≥n.

---

## üîó **Parte 2: Configurar Conexiones (Linked Services)**

### üî∏ **Cosmos DB**
- Crea un **Linked Service** con la clave de acceso proporcionada.  
- Define la conexi√≥n hacia el **contenedor de datos** donde se almacenan los documentos JSON.

### üî∏ **Fabric Lakehouse**
- Configura el conector de **OneLake** (requiere token de acceso o conexi√≥n directa desde Fabric).  
- Verifica la autenticaci√≥n y permisos para escritura en la tabla destino.

---

## üöÄ **Parte 3: Crear el Pipeline de Ingesta**

### üß± **Pasos detallados**
1. Abre **Azure Data Factory Studio**.  
2. Crea un **nuevo pipeline** y as√≠gnale un nombre descriptivo (ejemplo: `Pipeline_Ingesta_Cosmos_to_Fabric`).  
3. Agrega una actividad **Copy Data**.  

   **Configuraci√≥n:**
   - **Fuente:** Cosmos DB ‚Üí selecciona el contenedor correspondiente.  
   - **Destino:** Fabric Lakehouse ‚Üí elige la tabla en la **capa Bronze**.  
   - **Transformaciones:** mapea los campos y aplica ajustes como:
     - Conversi√≥n de fechas a formato est√°ndar.
     - Normalizaci√≥n de campos y tipos de datos.

üí° *Tip:* Puedes agregar un paso de validaci√≥n para asegurar que el esquema coincida con el esperado en Fabric.

---

## üß™ **Parte 4: Ejecuci√≥n y Validaci√≥n**

1. Ejecuta el pipeline creado desde **Data Factory Studio**.  
2. Verifica en **Microsoft Fabric** que los datos se encuentren disponibles en la tabla **Bronze**.  
3. (Opcional) Usa **Notebooks Spark** dentro de Fabric para:
   - Visualizar los datos.  
   - Validar el formato y consistencia.  
   - Aplicar transformaciones previas al enriquecimiento.

---

## ‚úÖ **Resultado Esperado**

Al finalizar este reto, deber√≠as obtener:

| Elemento | Resultado |
|-----------|------------|
| **Origen de datos** | Archivo CSV cargado como documentos JSON en **Cosmos DB** |
| **Pipeline** | Configurado y ejecutado correctamente en **Azure Data Factory** |
| **Destino** | Datos disponibles en **Microsoft Fabric (Capa Bronze)** |
| **Siguiente paso** | Los datos listos para transformaci√≥n en **Capa Silver** y enriquecimiento en **Capa Gold** |

---

üéØ **Conclusi√≥n:**  
Con este flujo completado, ya dispones de un **pipeline automatizado** que conecta **Cosmos DB** con **Microsoft Fabric**, sentando las bases para los siguientes retos de transformaci√≥n, enriquecimiento y an√°lisis de datos.

---

# Soluci√≥n Reto 01 ‚Äî Ingesta desde Cosmos DB a Microsoft Fabric (Capa Bronze) + Limpieza B√°sica

Gu√≠a paso a paso para ingerir datos desde Azure Cosmos DB hacia la capa Bronze de la Lakehouse en Microsoft Fabric, aplicar limpieza inicial y validar resultado.

Objetivo
- Ingerir datos desde Cosmos DB usando Dataflow Gen2 o pipeline y aplicar transformaciones b√°sicas (nulos, columnas, formatos).

Requisitos previos
- Conexi√≥n a Cosmos DB desde Fabric (ver `00-Solution.md`).

## Pasos

### 1 ‚Äî Crear Dataflow Gen2

1. En Fabric, Data ‚Üí New ‚Üí Dataflow Gen2.
2. Selecciona **Azure Cosmos DB** como fuente.
3. Rellena endpoint y key (o selecciona la conexi√≥n ya creada).
4. Selecciona la colecci√≥n/contenedor que contiene `products` o `creditScore`.

### 2 ‚Äî Dise√±ar transformaciones b√°sicas

Dentro del dise√±ador de Dataflow:
- Elimina columnas no necesarias.
- Normaliza formatos: convertir fechas, cadenas (trim, lower), normalizar decimales.
- Reemplaza o marca valores nulos (por ejemplo, `unknown` o valores por defecto).
- Filtra registros corruptos o incompletos (si aplica).

Consejo: agrega pasos de validaci√≥n intermedios y usa muestras peque√±as para probar transformaciones.

### 3 ‚Äî Destino: Lakehouse Bronze

1. Configura el sink/destino como la Lakehouse `Contoso_Lakehouse` ‚Üí esquema/tabla `bronze.sales`.
2. Ejecuta el Dataflow en modo de prueba y luego en producci√≥n.

### 4 ‚Äî Verificar y documentar

1. Abre la Lakehouse y revisa `bronze.sales`.
2. Verifica conteo de registros, columnas esperadas y formatos de columna (fechas, num√©ricos).
3. Guarda un registro de la ejecuci√≥n (logs) y captura de pantalla.

## Validaciones clave
- Conteo total vs origen en Cosmos DB.
- No hay columnas que deban eliminarse accidentalmente.
- Nulos manejados y formatos normalizados.

## Siguientes pasos sugeridos
- Automatizar con un pipeline (schedule) para ingestas peri√≥dicas.
- Implementar pruebas de calidad de datos (Data Quality checks) antes de mover a Silver.
